### 1. Introduce about Hadoop

![Introduce about Hadoop](https://lh3.googleusercontent.com/8k9QRSkFIVdaTh6Pw9oNFyLJYkoIkx9A59zWYQY-JlsEFQK-I-7AGn4nnkiovQu6E2PYzpmGMWN7LWEMhcDqT4WRG7UHbPN2k2uIz5VDovpZDCWq0-9WoS4cN4ulvtE3q97gFW231fZLK4WaccOHxlqs1UZKudg4AAGjCXsY48DxBDGr6wSUEsAAJuXhneozk0wsmhnM6P8AASOg4MAmX_bN3-DyuaYnxOuf6LP0uvuPJenxfyr483BA43agVzx4RW90vthQe02AW5Lr_l15JnqcxqVRRTgRAcu-JN2mukCzj2KiKaPLfGqSvbdkO9BlkqN253Hh6KzuzCyVFeOeEpT9ykfc8C5B_-91z8HhqtYW55seuL7Rx9rNguZ1t2WRtmak1gxyiEVO-VNtQrIOSAz8NyDRCXGcTOy8O0M1XVZ6DKaKG2lfppGAKhh4SpeN9HgiDytCSfM36cSGsLJpmvQWTlmFfO_cHfKqB9ten_8Yy8re2ZRXvAT4MlmyGv4ovskYATDDZUlLspzIgN9sP2-s3Zxv6PbYwZ4DW-lahIcPkdVhHAMfUSwhvG4F5odjOD97aplRq9yidJx2WBi7iyNnaxDzW17iuTRpxlE=w816-h639-no)


### 2. Install Hadoop 2.7.0 single node cluster 
### Part A: Installing dependencies  
+ __Step 1: Login with root:__
`sudo su`
+ __Step 2: Update the system:__
`apt-get update`
+ __Step 3: Installation of Open SSH Server__
`apt-get install openssh-server`
+ __Step 4: Create user and group for using Hadoop (optional)__

  + Create a group with name is `hadoop`: `sudo add group hadoop`
  + Add user `hduser` into `hadoop` group: `sudo adduser --ingroup hadoop hduser` 


+ __Step 5:  After the SSH Server is installed, configuration can be done by editing `sshd_config` that reside in the `/etc/ssh` directory. It's important to note  `sshd_config` is for the SSH Server while `ssh_config` is for the SSH client. Create a backup copy of `sshd_config` that can be used to restore your configuration by running the command below:__

    `    cp /etc/ssh/sshd_config    /etc/ssh/sshd_config.factory-defaults`

+ __Step 6: Open `sshd_config` in a text editor by running the command below for Ubuntu version:__

    `    cd /`

    `    nano /etc/ssh/sshd_config`
  
  + *At this file, disable password authentication by changing this line in configuration file:*
  
    Change line **`#PasswordAuthentication yes`** into **`PasswordAuthentication no`** 
    
    ![disable password authentication by changing this line in configuration file](https://lh3.googleusercontent.com/XuU6nWib6fBYx6k119K_A0jYEEoEDn26ABd2qu8Bo9sU5qL2a_agnRA8QU0C_IPiaCRipO3VRa0A_r3LejEo25u7adim-b-yxN6AZ616nQIW0CcOOVj7j7NLmgc76iFTsMH6au19iYnYu6l5R_C0AG0GE0AaCn8Xl3j0yJkJC3lGlRZMKlI5pgxYRxZIKjS83dvw-TvOMrHQBBybV_gqeUAioM8eIr0Ao37Qm5v3dWBdc67dS_K5bJAACvkXbLdVZuwSYGnYaWYtdaBD3NZw2ORK52DnZc9z-nCrsJNvQEkZ_i2rEtGPND0Qe7_zvCVdN1nwvjAM-Kky-Zrgne4AULpTHBQw3gjZAEd-BBe0o4m6RAY49vwC93C4YFGUPCTj_19R9EmsUjbWoZAvOcoM_JG3EvCuPX6TgEXTfJ4J-A9HWF_2cag9sORx1HE-DYJSN8KEvH4Ndin-ks2E7XSNRddzT9kbJYCg53G0ML30NaFLjWCAGXAbUj06Oq-9jKcd7PQAichbhaKDvJRuw5tNciUUvmC1-nnWIiqVDSvy5KOzblcECMmbWgEpLr5Oldyvr1BCiyBFM3q8cfn1ES3-bkd_beZDiASRXqpoXMfrvQ_Q_4pCxNqxjZXAXlxWVy0Vf_YKUfrKWq3Nz4XjV2L4wq-kZGwHzcf8=w698-h237-no)
  
  + *Go on adding the following lines to the end:*
  
    `AllowUsers hduser` 
  
    `PermitRootLogin no`
  
    `PubkeyAuthentication yes`
    
    ![Go on adding the following lines to the end](https://lh3.googleusercontent.com/Ajx8_yeETbhLd5d0zv9cpXCyGUxfhUtzMq25Igjf--vVolCEy7ErM3fWs8IzYoohMpI44Xt8eF9FaG_7YYK8ZpHVDieAq_onJCYQezxyIoOzBveknHLAviukCrG_UrGAYaHKQvFcv5u-pYsoMPO5w8gO3hIz4euU-Z9mu3O6Cy__VNSqH4hqT0lezec3Dd-qX5Coa4MLDV-gNXDhyvl1g6lI73dYikbbwtwJpYxTri4FlHuMGP14YgwOXzzy7tbfIsB1PLqFn-WLrxTu-XIUbneSrbzCCkJVSZqgaf2g0zGE6JibNkw-l9M2VWsrH52w4aHq6q6Yqx5BkUdpd45R1u-mBGekYSRtQ2VHGOeVC4cLgvwYcD61OxSfmpag3M2LKjQ9gZvPJetTF72QNzI1yABtbsDYm0Wue79lMZkGfWxHjipbFQDTMa2pJXUF2RHL4L7RKqo6DQy3SQ04vNw6Iwu3jCYVY5QnZNS2IHedX6UIIsYJlQBlJ53E0SUYENT0pbWK5V1R1jmT_enJtoPWSyxJRres14QUl4O4J5NzBzBXvB5pEoxBgDEaVQQ1Be7LaZXjF-5BAQ96PQMOZqqYdW2WtzS3YRhav-4QaSUJcSDNZxBkaYJrYeHVKKNM69scFgwVKYKN7BrpnHaBodP-rCeTpUHVyqI7=w670-h371-no)
  
  + *Continuous changing line **`#LogLevel INFO`** into **`Loglevel VERBOSE`***
  
   ![Continuous changing line #LogLevel INFO into Loglevel VERBOSE](https://lh3.googleusercontent.com/gtUWJTMOSoJaQKFkzP9vKSCywbnqGGCXtFYP_RVNasmJNUzoNdMUiK6zVf45oIGVdFnF55DW62X_Xw0thgJg2h3GrNfwllZFKpCLKpJAyp5JuSQZul8yZ3Z951Bq55CDcHoglveEhWam8u2N7NpUBplfw3_jh2dMsXp0GjqbESZ8NT5HC9dMNka_tNgOm5TtiWwiLWoksIZjEsOoYQ46OIzH_qEItWgh7yWReYE7ZsLg0u7GozLVz507XDaYlqBdzipd6xT77ROiswiQ3Tniez7C9RKmmg88nItFfBKz84vgsNNMBpYx0ZfAyLPhip56Eg20oruMT1T6dy53vDV-xQPE4UzwGvnddrALYcObnMHjm7j8ohJ_nZDXgz0mTkQldd3Rn_8nlk3_V2laN2C5ZwXnBnnY30jFxKcEbwsAI38RHXlR38gcAyMg_Aa-wyUbqBegXA5hyOkI4MhXQHykdJKbgWuAe1_w6Iulhe7lk3PJsdCu6Uvdp_WztdBSdoOlKFlo1JOQtLiujp57HMMcfTjzk_t-lM78f41kR64aExGF8liRcahgg7QGv4-43Bvc8XhECt0JI_d0lyQsyrAyKB-wo5Mx8fYNLajjkWkkNo4WNLt14DQGw854xkopGE_fSJ7Pfkid0ahfU6L6uLNX4Rtpg2ByE5xH=w662-h330-no)
  + *In the end, save and restart SSH Server by the following command:*
  
    `systemctl restart ssh`
    
+ **Step 7: After SSH Server is configured, an SSH key for the user `hduser` is generated by running the command below. They create an RSA key pair without password. A password would be required every time hadoop interacts with its node so we can save ourselves the bother of being prompted for a password every time.** 

    `su hduser`
    
    `ssh-keygen -t rsa -P ""`  
    
    ![create an RSA key pair without password](https://lh3.googleusercontent.com/Y90Yuy21Gg-lv7aFfJtmoERRpQeNEGrzfTNdMP_Uvs9BpQTPop23gubBxbDX5AoQTm53DOIXSubUpd3Fj1qehRbFwLoMDy22ESKqKYca3xDy55tqYX0c1OhyMCKzOH3f62TUe0VARKfVQG73-LjI1Y4TZIa37dHiGjxEX6858ms_cJmieLhHyopkLjhaeo1HPvYA19NULqFYY1rDla8eXdMqTrEIC3aTq-Drnm-N8300Y74QHOiJmEs8jyQ9uE9G_RQnLJJOix7oHrlWtwm78Kf-95vCyfK6tNEtEiai2Q2Nyjt_xPhSi7jFmStTRfUxfDr0ej9D2iZbt1PV1FemH8l71VMpwNVp5dIcU2uCvOja1qAk9O-rEh3FOoW4LPTaN9mwPoWcUa4mdpZo5aP-mJ7N45P9dto7dacJyJl9yGzO3oJujN2xInSCd3BQhswJuwBnUgLXu9jyogCs3uHMElicYz0tReyOrg2dcXH11CHXmtonZsq47l2LEPpvEId0NhRTDSe0weOcIUkjIVks8zQkeyueyTmW6hNMXI9sKzT39cwmnoPS98TNgD8WAo_-caJOJnPTB74XTtZn7cSyE8ddr8wPNsgRo2N9Hj0lsmJeahIKGJtcqMhg8Bw2kQFXX9i5uTDH0bLlGWHVFBiRrHOW3M_YoXJn=w740-h436-no)
    
    + *After the key has been created, we use it to enable SSH access to the local machine by running the command below which adds it to the list of known keys*
    
      `cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys`
    
    + *Testing localhost:*
    
      `ssh localhost`
      
      ![Testing localhost](https://lh3.googleusercontent.com/awQPSGBv6ebL1I_O10Way6dRxONppFxZD1Rk7NSBGcfgjwgFDlyRentj_aVeMbs3uyNX6J9vH8enq0kErH2e_gG75YZalg9yrSiTtrvM1vrLkEzBD9jiMne5BTSu-qFeSThPpKlfj8nPYJkZJQwiBDShHtlyFyPJkPzLu38VyAFngbDQ6EBa5bkJv4bKcw0PngohPIRJo0WL4ZUG9732m0Dzz2wZ4YkCesUycpAsBaqVIWPMQ2U_lWK8tP19h6MtbPRRYW94q-bWba1vmmvijQf9rWwjGAljHU8sKsnM8oOZ4a6bfdTcm5lBe8Ives_vgQiZ2cjD-byHafkfhU3vNn8PlUs5xO7WUkGIFFvSwuFWEe04cx3cc6LsYB_cEcx_T3uh5MqEq6KfSF7kYc6g96G8_2K0SzqcWLl4r8WoiQUSfR07C_oqUmGq31Mg_56fw2jGk48NbK9iCBfuI1UkY1MDWDjUigrEM6DrYxZqKY5eCws5tMx_2Uf89-NdIvDM2bv6kl7bunAPyIqEPUkW165ilL0NvuKc2xswftgEv4eNj8lY9o98Dfr0bsO1vCzNbF2b_XkCVW1SSsdUowBnaYd5doy2DQx-81SsskDTI7GecvTAOidDfqNqCT3LW2JqQeLWlJAvq4d162lsWfDiKJzhcOt-pgCn=w491-h295-no)
  
### Part B: Installing Apache Hadoop

+ **Step 1: Adding Java repository and installing Oracle Java 8:**

  `add-apt-repository ppa:webupd8team/java`
  
  `apt-get update`
  
  `apt-get install oracle-java8-installer`
  
  `java -version`
  
+ **Step 2: Downloading and installing Hadoop**
  + **Login with user `hduser`:**
  
    `su hduser`
     
    `cd Desktop`
  
    `wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.0/hadoop-2.7.0.tar.gz`
  
    `sudo tar -xzvf hadoop-2.7.0.tar.gz`
  
    `sudo mkdir /usr/local/hadoop`
  
    `sudo mv hadoop-2.7.0    /usr/local/hadoop`
  
    `sudo chown -R hduser /usr/local/hadoop`
  
+ **Step 3: You need to edit .bashrc file for the user `hduser`, so open it in text editor by running `nano ~/.bashrc` from the terminal**

  `readlink -f /usr/bin/java`
  
  `nano ~/.bashrc`
  
  + And add the following statement below:
  
     `export JAVA_HOME=/usr/lib/jvm/java-8-oracle` 
  
     `export HADOOP_INSTALL=/usr/local/hadoop/hadoop-2.7.0`

     `export PATH=$PATH:$HADOOP_INSTALL/bin`

     `export PATH=$PATH:$HADOOP_INSTALL/sbin`

     `export HADOOP_MAPRED_HOME=$HADOOP_INSTALL`

     `export HADOOP_COMMON_HOME=$HADOOP_INSTALL`

     `export HADOOP_HDFS_HOME=$HADOOP_INSTALL`

     `export YARN_HOME=$HADOOP_INSTALL`

     `export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native`

     `export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib/native"`
     
     ![add the following statement above](https://lh3.googleusercontent.com/-XtMnHqBWoBCuNywfB29a2NNo4h_jcCp6IuRB-onKBHRRE6L1wsqOYBV8I32Bfdzzg8miXDVAp71nY7j5W-jW7hsNWFtKpRuJI-i8zyq_CcYAhLMbzUsvSQuf-k0T1_H2NHqxjauOmgPo2WQcXVeH53jg6eZe6P4dYOB4A0RwZ6qrruTo8I6Mq9wYdfpgtv8amaMu6o0DMm6KM9xOOBfPmnHedme3kD5RW1W04-DcP5Ykl0-l0NhRdgkkIzWM9tQRj5ZvD9HxGT7TblmWMdS10oq80H-LqKirRGwh-1N842ITvcUNiHLpfxTzvTGppAoJSKpM5uR6V2-chcVd4wt34uhfJtofax5KEE8p4nSUbHh4CD8yXazaSHpMOt5yBtzV4ula5SvC-D-HbPoZ6L-_S306JV3XhFnoGt5QB172JSvMHxXPF4h5U0E2rrVEx9VRPWnycK4XejjQnQggoEmq0ESvsAZJaigL7mFQBoGZREInJP_9xmyjNGuo7ijlcuSPwnUIuIMdlcRf5y3inow7KjQUPJArDGlXnvL1v297G8493USiE4xP7q8n5e_TQBa5mRKPZUFPY081NMFEwEBXLNuj3mhbVWrsiW4iPBXhjnx2o9fGLS9ICJwBXWpxtc53ZnjsKtMPBANHwnF4hL-lqkCkwKijFkf=w669-h364-no)
  
  + Save the file and exit 
  + Save the changes to `.bashrc` file:
  
    `source ~/.bashrc`
  
+ **Step 4: Exporting `JAVA_HOME` path**

  + *The directory **`/usr/local/hadoop/hadoop-2.7.0/etc/hadoop`** contains configuration files. Open **`hadoop-env.sh`** in a text editor and set **`JAVA_HOME`** variable by adding the line below. This specifies the java installation that will be used by Hadoop*
  
    `cd /usr/local/hadoop/hadoop-2.7.0/etc/hadoop`
  
    `nano hadoop-env.sh`
  
  + *Adding this line:*
  
    `export JAVA_HOME=/usr/lib/jvm/java-8-oracle`
  
  + *Save and exit*
  
+ **Step 5: Create a directory that will act as the base for other temporary directories and assign it to the user **`hduser`** by running the commands below**

  `sudo mkdir -p /app/hadoop/tmp`
  
  `sudo chown hduser /app/hadoop/tmp`
  
### Part C: Editing the XML based Hadoop configuration files

All the hadoop configuration files reside under **`/usr/local/hadoop/hadoop-2.7.0/etc/hadoop`**

The first time, please you access to **`/usr/local/hadoop/hadoop-2.7.0/etc/hadoop`**

`cd /usr/local/hadoop/hadoop-2.7.0/etc/hadoop`

+ **A. Edit `core-site.xml`**

  `nano core-site.xml`
  
  + Adding more the statement in file `core-site.xml`:

    ```
    <configuration>
            <property>
                    <name>fs.default.name</name>
                    <value>hdfs://localhost:9000</value>
            </property>
    </configuration>
    ```
+ **B. Edit `mapred-site.xml`**
  + You type the following command below:
  
   `cp /usr/local/hadoop/hadoop-2.7.0/etc/hadoop/mapred-site.xml.template    /usr/local/hadoop/hadoop-2.7.0/etc/hadoop/mapred-site.xml`
   
   `nano mapred-site.xml`
  
  + Adding more the statement in file `mapred-site.xml`:

    ```
    <configuration>
          <property>
                  <name>mapred.job.tracker</name>
                  <value>localhost:9001</value>
          </property>
          <property>
                  <name>mapreduce.framework.name</name>
                  <value>yarn</value>
          </property>
    </configuration>
    ```
+ **C. Edit `yarn.site.xml`**:

    `nano yarn.site.xml`
    
    + Adding more the statement in file `yarn.site.xml`:

    ```
    <configuration>
            <property>
                    <name>yarn.nodemanager.aux-services</name>
                    <value>mapreduce-shuffle</value>
            </property>
            <property>
                    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
                    <value>org.apache.hadoop.mapred.shuffleHandler</value>
            </property>
    </configuration>
    ```

### Part D: The hdfs-site.xml is used to specify the namenode and datanode directories. 

+ Before modifying this file, we create the namenode and datanode directories.

+ Please you type the following command below:

  `sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode`

  `sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode`

  `sudo chown -R hduser /usr/local/hadoop_store`

+ And go on, you have to modify `hdfs-site.xml`

  `nano hdfs-site.xml`

  ```
  <configuration>
          <property>
                  <name>dfs.replication</name>
                  <value>4</value>
          </property>
          <property>
                  <name>dfs.namenode.name.dir</name>
                  <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
          </property>
          <property>
                  <name>dfs.datanode.name.dir</name>
                  <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
          </property>
  </configuration>
  ```
+ Save and exit
### Part E: Final step

Format the file system by running hdfs namenode. Format to initialize the file system. Open terminal and type command below: 

`hdfs namenode -format`

![Format the file system by running hdfs namenode](https://lh3.googleusercontent.com/LiJhqA12uVsmKhyJzUjG5Y4lXGYnuGBWAR7OlwAKKVux_rfslwW-7ZnW9s5Lzj2hJi6Rnj4GuU6Bc3p2ifKGQO3XKeW_E8mabqRl8IAnNoUMTQMZ86Uu01yweHVk0ekOn7N2q68DVtjpsVm8xJiqUM-XMPZ3e6wb23MCvyoDI2NgroA2WrNbhtsb4tauZ9QAZg7Om0akHkPNV51636011zmgZmLv3EjRW6X8485lj24d1mabMAiquSuSZ2fzqqIOxPE3XEOTlWUf9l1uDprs6rdOvZjquJ448Y5jUq-O_wxGlWeTUMJfWeYj0wQyHZEX-EcHU1adacOnk2BSlVNY0Kzfa8nt7ggQXcpfH_1xAcU-pxc4BeYYBmZKb1ZonukkMBLT_yRF713huXsvOyzFtIzP8u3ymTgnwoRLdhooF-2cSLveiSt2_W1-AmBLqu-sI5LrAcqoc1r0mzi27I3fJOJiDwBJqrliwhSvE2uh6Jh9AGYX3HK0Zef6rPcnvkPWQLXFuQmjuVjskhiz2eGkbMILGN_WLVuSre_NZpSbZVNkVhwxGlPyHiYdTozfWEO6tVn8GrWa9Pq1i63llqfJdj3cuTX3sd1Jgz1cgTIiXbl2-4b1fDFlKhhzJVGgcTbauokLgE5DJcOy5yxLujifZ3bhCcWyAR13=w1299-h467-no)

Start the single node cluster:

`start-dfs.sh`

![start-dfs.sh image](https://lh3.googleusercontent.com/X28Lzaov6e-0b4AtvYB6c-IjPQgJS6JGypo9C7eC_sZtgpbajaGvJRlWCG0HPw4PH-8FtPCpw1RWGojLXlfJ_qS__NjWMB9wZQTIJ-ioWBbCLgif0cU24jJQr7F5fljASkY9gA7-1gdtOFpkQnbd8RvrPS6lsa6HsDG4y3cdGhEWe0GppwH9oe-EY1tL8HoLsaDqCQ8yaLvFC0LLHUsYMu0AcjuhQXNrOBBSkU1IqxypKmXHzH9Tnjvhm0XGN-z-uj88tIsodZrlMcmbQh2Y69F_hVSLOLxKjn6f6kprggiIX_wA6YzIi1ra_PuyS77aCYXkSCwjNhJBhZxg7Og16nr2j9BxELfc81xvouNUoEtPPg-W09YveKqxf_NzOgc-pHBDMC3bquhD7fhZEz-ftap3trtcFweydN61MLfIQI_Z7t7P2Kc49A86h8qoT0B1tqSKtSreadkzkTW9zGyV0czQ1a3OkFlefqJflawj7NwHNItJQ5Gx__vv0otwDHK5pozZxRxQo8dndqd63XvQ2tdJMrNQPB6R764nZicvnqOhLTasNA4XVmz4J_qM2n19R2LRJm4FOXXvveonvlauHdzOWZEvHxMzzSYkfshhNF1g0EBJx3sM4xHzw5gi91eT6ZjZUX8qg22rIhP_HCZ7SfNu4RDLYh7i=w1299-h216-no)

`start-yarn.sh`

![start-yarn.sh image](https://lh3.googleusercontent.com/QTo1Pg9r-l49-Ah8J6oOcCNoKGzj5h73SzJcHu0isl-XNCsa_TXtfARyQCgfUGlf5rwHJsXKn3EA1b-l4Uwbwt4qPp-Xa9Iwww-3RibuefuCMgfqyZ-Ho5h8HbrTRATtt7IwD8J9tD-gnxoDUD8_-Stiim5WEDCdM0oX-v-UzVeET1eFsUpeogTby6LzZB8S_bSDqX5nIOVRi6Y9DIryyHs61KSi-0yhRm7IfI3dDHBqivS7GZk4b0KjQt9jq4kADZjAD75s9smsaF62qsrjASqnwxf4TEVVgNDTkZROQua1XD0HBJJrL7OIC61kqJMBl3Lbhvjoi52U0Z27bpsB4XodqZyXuI-2PMfkeqJlReIf3nCpl2XMVa3_KG65rDYkBkvUIVqOHAYljPvoyXjK-wPaLvJQUbS_zayNRgydh6evt_-DcETnUs-BWIF6c3RhZ2QUR9rQlu0MGl1xp31RAve6d6ncmfbzWQDMYIFpMJnJbRmZCadrhYSbzCLD_B9du1Bn1O3ofLCuWpgQhPvCBUc2nMDaeMUOF_WZsWo9q0BLZssVS1LuIIhVqUn31XSG0WYidv1Un22uT_YnMDTcH2ROSJ-NTn2lnngcBKlfiVZCxbQcmLVYk7icZPa9Q_653_ByvXB2lmqOISV4i9xYODE77e1bSALj=w1299-h102-no)

`jps`

![jps image](https://lh3.googleusercontent.com/mYVELD248oBKoEwJ_JfJ_dUstOulgqs1Quhd8xW--D86-Q6DO_E3oCRBb_oOpHSAg5mNqJIbx5qsZxm7Tae5lVswF6d_3CiOU4GZ7qBdZsgraQyljeeQxB4f-9Cm0sA3QmTSec0gQe1d0s8Sj9T27rnVOmbj3ex99GE7IS3dKnJw4H-8rUAOW2TqAsVV3thCx3XFySSmuHF6VQ5YFOnF5DJuuM0Z6mUUYBvJFSZYKtZpiphf-876CwpEe7MyeiAVTGtaLA6TtO800vwkQQEvEO3LytaWFYVWm42xy9BASfgdCQhho2u4ZI6J7kBrxCOBd98CODTkUyzUDFxcrCr91iLTKPr3lZCE45qGyyWqZwIhu2eAo_w1nDUnynWZRRr_e_9aJ6Gg0svFfz9NYFfxgYrXjyXssbD4fMWGOcfLHbsZOsFKwqrFy8NrU09F2Pd_MlWfWtJoPFux-d9MlfazY1plrIJdWDOkDjZgDPiZI8si1uLoJEMWbmem_4hxJpbnI3FO1WgHNMnHX0TWS1FdeveXlln3EgrEb0lEv86sidjNK-nDxLdf12dhBGdfYOYkODTKYemDz54tVZqk9iYxfjp-jZY2VFNlmmxs2KZY1df9LYRrbsQ_3Kmkin7IRzdX4fyBeANqWZEeIv_uxtijGE7lALpio3tw=w715-h155-no)

Open web browser to see result:

`https://ip_address:8088 (main cluster)`

![Main cluster](https://lh3.googleusercontent.com/KLnEciSKGmpY5mo7_XaHbrriDl52t2hsLKWb9m504piC-JDgLO0esUZpnA1a_6hYVz4gfh-f6Hm-yuTUMyEkxC1mHE_UTxtsTziwJm9DQUtdcDhM6WCZ1XnM3DPaq0jNP49Y1wYZfhXQiN6OIpjwu9Rq1PyKquEqge6E_-o5aCcpWR5y35AtHDBPoh58QmajdPgyNVqUDTozLStLqw84WgTOORigg2FZXiiJPjNUsoUkQKaxu6f9gxU7eEhLvF4-OTTrAdssOklojRCM35_3UL4mvm0kaJEjTsUjukLoIMjNL9uSdU0QRXClvH8c8az0W6oxst8CeHXywnq9KK7lCXN0g6qC2--jOX-fQI-GT2qEq50mrDDrg906naHRX5tH-hB8TgTQJBkPO-M7sSGROso1eZgD4ek0IJSCKAPEceFTSW7cdWv3qT22X4C1181rSMxUghDfhs8gN4tfUghZPgy47xeMtkjzr62-0Ihq62xZLG7D9Z31wuJ2OhT18nEEjd5cau7kbo3C4trhXk2Di0QYurmTi3jh6KU657BcV3dedy0Z3BsgWBcZATQmWTrDpaaAeVPtp5pMxDWSi6eRSc2zAe3xxW7kfYdSkKFHj5EGEx5isC0vYRHbVq-0MVkuWxovGrkheyU3hvbP3M90NTneYIa1X0gC=w1299-h448-no)

`https://ip_address:50070 (see detail information)`

![Detail information](https://lh3.googleusercontent.com/Fui6J9-l-b5INBHUybtoYLHmr68KCdGUgIQhMkG4f08Oi0hcfUR2fkZuZnJN3UzigldZ6PQhdN7fCGqorAuZ36HF1sVCJ-vD3qKuBWcwz2gU0Kv4S1E4SBPDngVD0RFT4-NpOcCho3CYE_kTI5v8_NGkC0cROpPze9MlJd5NCaqjp5NaBizNYBaBGzAiiAijGxPZE_9pVlOBiZuAstg5XU1HJr8CUoQDttnBc43Ccl_zVUywrd9jYsQ69mwosAwQSE6qAp_9wQuT36YRTE0ry4IUWkGLH6Uh7T8KnggFmFUtzPYXMc0Qp8R6u6i1LfckAyc9zzXdqxdSJgStXJqf4qCF02vKiqRGij6o8nZ2pZXZMaQoEdxi6Hu-JA34amQJFGwPluLj48zvAEhwjfLxaUBLg9Nw_qJ36l0SjDJbfyGuLb8nep9FkRP7QtJDoWGmJpKqYtN22HWtd05oZXF0ogAnmYmG6OENuYeuXCz7MghceHSdomvV1-Uza7M7KXOj2KwkcLswYEk_kPfd207HJ5Rm9Ou9siVPIrN2AxFBnB6uucprGoRS5PjBScnPMVYOsfsEXh-eRwud8R6_hyWl9Thodrx7kjpGpUWoC7gaK3WVANrU4GS2MBS5vu2FRKSkqMr8n5EEQBg9DTk1rVQPzYAuHIQ1bC9s=w1184-h647-no)

### 3. Built with:

+ [Hadoop](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html) - Setting up a Single Node Cluster

### 4. Author:

+ **Hau Trung Nguyen** - *initial work* - [flightstar](https://github.com/flightstar)

### 5. Support:

*React out to me at one of the following place:*
+ Facebook at [link](https://www.facebook.com/haunt.hcm2015)
+ Skype: haunt.hcm2015
